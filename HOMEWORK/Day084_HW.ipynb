{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Day084_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aWsPSIrFO84",
        "colab_type": "text"
      },
      "source": [
        "## Work\n",
        "### 請結合前面的知識與程式碼，比較不同的 regularization 的組合對訓練的結果與影響：如 dropout, regularizers, batch-normalization 等"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCA0cIDKFO87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n",
        "import itertools\n",
        "# Disable GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EIe4okQFO8_",
        "colab_type": "code",
        "outputId": "98da8ba1-745d-4ed1-b5e3-dc41154b2698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train, test = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAu740KkFO9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 資料前處理\n",
        "def preproc_x(x, flatten=True):\n",
        "    x = x / 255.\n",
        "    if flatten:\n",
        "        x = x.reshape((len(x), -1))\n",
        "    return x\n",
        "\n",
        "def preproc_y(y, num_classes=10):\n",
        "    if y.shape[-1] == 1:\n",
        "        y = keras.utils.to_categorical(y, num_classes)\n",
        "    return y    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ns_ZAJFO9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train\n",
        "x_test, y_test = test\n",
        "\n",
        "# Preproc the inputs\n",
        "x_train = preproc_x(x_train)\n",
        "x_test = preproc_x(x_test)\n",
        "\n",
        "# Preprc the outputs\n",
        "y_train = preproc_y(y_train)\n",
        "y_test = preproc_y(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBSm5U6FO9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "\"\"\"\n",
        "建立神經網路，並加入 L1 \n",
        "加入 dropout layer\n",
        "\n",
        "\"\"\"\n",
        "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], l1_ratio=1e-4, drp_ratio=0.2):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    \n",
        "    for i, n_units in enumerate(num_neurons):\n",
        "        if i == 0:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1), \n",
        "                                   kernel_regularizer=l1(l1_ratio))(input_layer)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        else:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1),\n",
        "                                   kernel_regularizer=l2(l1_ratio))(x)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "            x = BatchNormalization()(x)\n",
        "    \n",
        "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
        "    \n",
        "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94rHdQzkFO9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "MOMENTUM = 0.95\n",
        "L1_EXP = [1e-2, 1e-4, 1e-8, 1e-12]\n",
        "\n",
        "Dropout_EXP = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mgph-7rsFO9N",
        "colab_type": "code",
        "outputId": "77ff44ac-fb40-4b5d-e5a0-2432ded7cb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = {}\n",
        "\"\"\"\n",
        "使用迴圈建立不同的帶不同 L1/L2 的模型並訓練\n",
        "\"\"\"\n",
        "for regulizer_ratio in L1_EXP:\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with Regulizer = %.6f\" % (regulizer_ratio))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:], drp_ratio=Dropout_EXP, l1_ratio=regulizer_ratio)\n",
        "    model.summary()\n",
        "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"exp-l1-%s\" % str(regulizer_ratio)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment with Regulizer = 0.010000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,742,474\n",
            "Trainable params: 1,740,682\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "50000/50000 [==============================] - 17s 345us/step - loss: 148.7420 - acc: 0.2259 - val_loss: 15.6965 - val_acc: 0.1426\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 9.8272 - acc: 0.1975 - val_loss: 8.7172 - val_acc: 0.2235\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 7.9606 - acc: 0.2074 - val_loss: 7.9776 - val_acc: 0.1671\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 6.7992 - acc: 0.2127 - val_loss: 6.5297 - val_acc: 0.1674\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 5.8784 - acc: 0.2251 - val_loss: 5.4319 - val_acc: 0.2289\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 15s 291us/step - loss: 5.2032 - acc: 0.2312 - val_loss: 5.0479 - val_acc: 0.2127\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 4.7317 - acc: 0.2367 - val_loss: 4.6246 - val_acc: 0.2217\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 4.3016 - acc: 0.2448 - val_loss: 4.2443 - val_acc: 0.1915\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 4.0079 - acc: 0.2417 - val_loss: 4.0344 - val_acc: 0.2040\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 3.7489 - acc: 0.2494 - val_loss: 3.7265 - val_acc: 0.2518\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 3.5519 - acc: 0.2496 - val_loss: 3.6528 - val_acc: 0.1930\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 14s 288us/step - loss: 3.3075 - acc: 0.2504 - val_loss: 3.9114 - val_acc: 0.1561\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 14s 285us/step - loss: 3.1662 - acc: 0.2604 - val_loss: 3.2468 - val_acc: 0.2247\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 3.0432 - acc: 0.2629 - val_loss: 3.0130 - val_acc: 0.2750\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 15s 310us/step - loss: 2.9272 - acc: 0.2676 - val_loss: 2.8777 - val_acc: 0.2655\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.8660 - acc: 0.2551 - val_loss: 2.9256 - val_acc: 0.2463\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.8700 - acc: 0.2527 - val_loss: 2.7690 - val_acc: 0.2317\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.6829 - acc: 0.2782 - val_loss: 2.8395 - val_acc: 0.2265\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.6065 - acc: 0.2806 - val_loss: 2.5640 - val_acc: 0.3060\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.5683 - acc: 0.2795 - val_loss: 2.6681 - val_acc: 0.2093\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 2.5191 - acc: 0.2819 - val_loss: 2.5643 - val_acc: 0.2367\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 15s 298us/step - loss: 2.4634 - acc: 0.2872 - val_loss: 2.6570 - val_acc: 0.1957\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 14s 287us/step - loss: 2.4404 - acc: 0.2784 - val_loss: 2.5524 - val_acc: 0.2014\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 15s 301us/step - loss: 2.3925 - acc: 0.2893 - val_loss: 2.6021 - val_acc: 0.2254\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3673 - acc: 0.2918 - val_loss: 2.5152 - val_acc: 0.2153\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 2.3629 - acc: 0.2878 - val_loss: 4.3504 - val_acc: 0.1183\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.3377 - acc: 0.2905 - val_loss: 2.4188 - val_acc: 0.2586\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 2.3363 - acc: 0.2888 - val_loss: 2.5663 - val_acc: 0.1944\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 2.3227 - acc: 0.2908 - val_loss: 2.4480 - val_acc: 0.2273\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 2.3111 - acc: 0.2899 - val_loss: 2.4441 - val_acc: 0.2290\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.3025 - acc: 0.2908 - val_loss: 2.5969 - val_acc: 0.2247\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 2.2948 - acc: 0.2909 - val_loss: 2.7084 - val_acc: 0.1358\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 2.2820 - acc: 0.2960 - val_loss: 2.3535 - val_acc: 0.2852\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 2.2779 - acc: 0.2944 - val_loss: 2.8716 - val_acc: 0.1495\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 16s 314us/step - loss: 2.2712 - acc: 0.3002 - val_loss: 2.6390 - val_acc: 0.1982\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.2740 - acc: 0.2959 - val_loss: 2.5978 - val_acc: 0.2115\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.2747 - acc: 0.2972 - val_loss: 2.6709 - val_acc: 0.1594\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.2605 - acc: 0.3036 - val_loss: 3.1192 - val_acc: 0.1810\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.2582 - acc: 0.3081 - val_loss: 2.6027 - val_acc: 0.2304\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 15s 306us/step - loss: 2.2509 - acc: 0.3065 - val_loss: 2.7540 - val_acc: 0.1854\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 2.2600 - acc: 0.2977 - val_loss: 2.7864 - val_acc: 0.1570\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 2.2584 - acc: 0.3022 - val_loss: 2.8610 - val_acc: 0.1737\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 15s 307us/step - loss: 2.2530 - acc: 0.3061 - val_loss: 2.4911 - val_acc: 0.2134\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 311us/step - loss: 2.2523 - acc: 0.3041 - val_loss: 2.4241 - val_acc: 0.2489\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 15s 293us/step - loss: 2.2552 - acc: 0.3055 - val_loss: 2.4531 - val_acc: 0.2586\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 14s 285us/step - loss: 2.2454 - acc: 0.3062 - val_loss: 2.5084 - val_acc: 0.2140\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 14s 286us/step - loss: 2.2657 - acc: 0.2992 - val_loss: 3.1596 - val_acc: 0.1437\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 15s 300us/step - loss: 2.2383 - acc: 0.3125 - val_loss: 2.5584 - val_acc: 0.2322\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.2631 - acc: 0.3012 - val_loss: 2.4421 - val_acc: 0.2500\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 2.2691 - acc: 0.2978 - val_loss: 2.5683 - val_acc: 0.2155\n",
            "Experiment with Regulizer = 0.000100\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,742,474\n",
            "Trainable params: 1,740,682\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 5.5647 - acc: 0.2308 - val_loss: 5.0399 - val_acc: 0.3526\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 5.1520 - acc: 0.3059 - val_loss: 4.8806 - val_acc: 0.3881\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 4.9725 - acc: 0.3379 - val_loss: 4.7811 - val_acc: 0.3916\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.8454 - acc: 0.3618 - val_loss: 4.6750 - val_acc: 0.4072\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.7353 - acc: 0.3816 - val_loss: 4.5973 - val_acc: 0.4166\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.6431 - acc: 0.3904 - val_loss: 4.5205 - val_acc: 0.4232\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 4.5528 - acc: 0.4032 - val_loss: 4.4221 - val_acc: 0.4440\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 4.4706 - acc: 0.4141 - val_loss: 4.3413 - val_acc: 0.4541\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 4.3973 - acc: 0.4204 - val_loss: 4.2643 - val_acc: 0.4570\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 14s 287us/step - loss: 4.3250 - acc: 0.4257 - val_loss: 4.2271 - val_acc: 0.4511\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 14s 289us/step - loss: 4.2581 - acc: 0.4310 - val_loss: 4.1307 - val_acc: 0.4701\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 15s 296us/step - loss: 4.1952 - acc: 0.4348 - val_loss: 4.0973 - val_acc: 0.4571\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 15s 308us/step - loss: 4.1282 - acc: 0.4378 - val_loss: 4.0276 - val_acc: 0.4668\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 4.0696 - acc: 0.4385 - val_loss: 3.9751 - val_acc: 0.4660\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.0078 - acc: 0.4416 - val_loss: 3.9095 - val_acc: 0.4697\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.9560 - acc: 0.4434 - val_loss: 3.9156 - val_acc: 0.4493\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 3.8985 - acc: 0.4481 - val_loss: 3.8259 - val_acc: 0.4601\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 3.8435 - acc: 0.4463 - val_loss: 3.7700 - val_acc: 0.4589\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 3.7895 - acc: 0.4499 - val_loss: 3.6991 - val_acc: 0.4708\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 3.7328 - acc: 0.4515 - val_loss: 3.6929 - val_acc: 0.4530\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 3.6806 - acc: 0.4532 - val_loss: 3.6121 - val_acc: 0.4621\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 3.6187 - acc: 0.4578 - val_loss: 3.5625 - val_acc: 0.4743\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 3.5618 - acc: 0.4580 - val_loss: 3.5156 - val_acc: 0.4639\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 3.5221 - acc: 0.4573 - val_loss: 3.4282 - val_acc: 0.4848\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 3.4677 - acc: 0.4613 - val_loss: 3.4274 - val_acc: 0.4650\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 17s 337us/step - loss: 3.4226 - acc: 0.4622 - val_loss: 3.3694 - val_acc: 0.4744\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 3.3690 - acc: 0.4675 - val_loss: 3.5239 - val_acc: 0.3951\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 3.3386 - acc: 0.4605 - val_loss: 3.3418 - val_acc: 0.4434\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 3.2941 - acc: 0.4629 - val_loss: 3.2622 - val_acc: 0.4659\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 3.2393 - acc: 0.4650 - val_loss: 3.1793 - val_acc: 0.4743\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 3.2065 - acc: 0.4607 - val_loss: 3.1689 - val_acc: 0.4684\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.1521 - acc: 0.4668 - val_loss: 3.1093 - val_acc: 0.4606\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.1153 - acc: 0.4650 - val_loss: 3.0710 - val_acc: 0.4747\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 3.0718 - acc: 0.4638 - val_loss: 3.0010 - val_acc: 0.4835\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 3.0349 - acc: 0.4655 - val_loss: 2.9936 - val_acc: 0.4683\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 17s 340us/step - loss: 2.9804 - acc: 0.4698 - val_loss: 2.9356 - val_acc: 0.4759\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 2.9548 - acc: 0.4680 - val_loss: 2.8923 - val_acc: 0.4838\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 17s 336us/step - loss: 2.9035 - acc: 0.4669 - val_loss: 2.8725 - val_acc: 0.4724\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 2.8538 - acc: 0.4732 - val_loss: 2.8656 - val_acc: 0.4610\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.8156 - acc: 0.4734 - val_loss: 2.7480 - val_acc: 0.4879\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 2.7782 - acc: 0.4717 - val_loss: 2.7646 - val_acc: 0.4722\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 2.7490 - acc: 0.4754 - val_loss: 2.7446 - val_acc: 0.4660\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.7099 - acc: 0.4734 - val_loss: 2.7368 - val_acc: 0.4430\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.6623 - acc: 0.4813 - val_loss: 2.6616 - val_acc: 0.4726\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.6455 - acc: 0.4759 - val_loss: 2.6133 - val_acc: 0.4811\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 2.6013 - acc: 0.4781 - val_loss: 2.5833 - val_acc: 0.4774\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 2.5673 - acc: 0.4808 - val_loss: 2.5248 - val_acc: 0.4913\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 2.5481 - acc: 0.4745 - val_loss: 2.5795 - val_acc: 0.4621\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 2.5154 - acc: 0.4771 - val_loss: 2.5350 - val_acc: 0.4604\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 17s 335us/step - loss: 2.4773 - acc: 0.4773 - val_loss: 2.4533 - val_acc: 0.4831\n",
            "Experiment with Regulizer = 0.000000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,742,474\n",
            "Trainable params: 1,740,682\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 18s 355us/step - loss: 2.3260 - acc: 0.2336 - val_loss: 1.8201 - val_acc: 0.3589\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 1.9698 - acc: 0.3082 - val_loss: 1.7491 - val_acc: 0.3689\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 1.8453 - acc: 0.3454 - val_loss: 1.6997 - val_acc: 0.3960\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 17s 339us/step - loss: 1.7697 - acc: 0.3703 - val_loss: 1.6414 - val_acc: 0.4208\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 17s 338us/step - loss: 1.7243 - acc: 0.3854 - val_loss: 1.5880 - val_acc: 0.4399\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 17s 335us/step - loss: 1.6869 - acc: 0.4000 - val_loss: 1.5693 - val_acc: 0.4429\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.6575 - acc: 0.4095 - val_loss: 1.5569 - val_acc: 0.4479\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.6369 - acc: 0.4144 - val_loss: 1.5354 - val_acc: 0.4530\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.6097 - acc: 0.4234 - val_loss: 1.5235 - val_acc: 0.4524\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.5971 - acc: 0.4285 - val_loss: 1.4953 - val_acc: 0.4657\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 15s 301us/step - loss: 1.5830 - acc: 0.4324 - val_loss: 1.4802 - val_acc: 0.4725\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 1.5629 - acc: 0.4423 - val_loss: 1.4773 - val_acc: 0.4731\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 15s 303us/step - loss: 1.5527 - acc: 0.4441 - val_loss: 1.4576 - val_acc: 0.4764\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 14s 287us/step - loss: 1.5356 - acc: 0.4496 - val_loss: 1.4559 - val_acc: 0.4770\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 15s 308us/step - loss: 1.5235 - acc: 0.4533 - val_loss: 1.4473 - val_acc: 0.4776\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.5100 - acc: 0.4593 - val_loss: 1.4478 - val_acc: 0.4773\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.5011 - acc: 0.4613 - val_loss: 1.4546 - val_acc: 0.4818\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 15s 302us/step - loss: 1.4955 - acc: 0.4618 - val_loss: 1.4601 - val_acc: 0.4780\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 15s 291us/step - loss: 1.4879 - acc: 0.4659 - val_loss: 1.4772 - val_acc: 0.4648\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 16s 310us/step - loss: 1.4925 - acc: 0.4663 - val_loss: 1.4252 - val_acc: 0.4831\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.4742 - acc: 0.4694 - val_loss: 1.4337 - val_acc: 0.4836\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.4716 - acc: 0.4733 - val_loss: 1.4414 - val_acc: 0.4808\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 17s 339us/step - loss: 1.4689 - acc: 0.4740 - val_loss: 1.4267 - val_acc: 0.4867\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 17s 339us/step - loss: 1.4597 - acc: 0.4744 - val_loss: 1.4197 - val_acc: 0.4887\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 1.4472 - acc: 0.4814 - val_loss: 1.4476 - val_acc: 0.4805\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.4446 - acc: 0.4800 - val_loss: 1.3961 - val_acc: 0.4988\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 15s 302us/step - loss: 1.4426 - acc: 0.4837 - val_loss: 1.4130 - val_acc: 0.4916\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 14s 287us/step - loss: 1.4338 - acc: 0.4857 - val_loss: 1.4166 - val_acc: 0.4847\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 14s 286us/step - loss: 1.4237 - acc: 0.4902 - val_loss: 1.4290 - val_acc: 0.4978\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 14s 288us/step - loss: 1.4166 - acc: 0.4895 - val_loss: 1.4064 - val_acc: 0.4879\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 14s 284us/step - loss: 1.4039 - acc: 0.4959 - val_loss: 1.4266 - val_acc: 0.4880\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 14s 287us/step - loss: 1.4058 - acc: 0.4948 - val_loss: 1.4098 - val_acc: 0.4956\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 15s 295us/step - loss: 1.3940 - acc: 0.4989 - val_loss: 1.3975 - val_acc: 0.4935\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 1.3885 - acc: 0.5014 - val_loss: 1.4015 - val_acc: 0.4928\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 15s 302us/step - loss: 1.3929 - acc: 0.4994 - val_loss: 1.3859 - val_acc: 0.4993\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 15s 296us/step - loss: 1.3863 - acc: 0.5022 - val_loss: 1.4029 - val_acc: 0.4950\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 313us/step - loss: 1.3896 - acc: 0.5034 - val_loss: 1.4178 - val_acc: 0.4881\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.3818 - acc: 0.5014 - val_loss: 1.3877 - val_acc: 0.5041\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.3781 - acc: 0.5061 - val_loss: 1.3740 - val_acc: 0.5057\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 1.3716 - acc: 0.5066 - val_loss: 1.3824 - val_acc: 0.5013\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.3650 - acc: 0.5080 - val_loss: 1.4103 - val_acc: 0.4954\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 15s 291us/step - loss: 1.3557 - acc: 0.5119 - val_loss: 1.3681 - val_acc: 0.5094\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 15s 295us/step - loss: 1.3565 - acc: 0.5101 - val_loss: 1.3669 - val_acc: 0.5098\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 14s 286us/step - loss: 1.3455 - acc: 0.5175 - val_loss: 1.3661 - val_acc: 0.5090\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.3355 - acc: 0.5207 - val_loss: 1.3597 - val_acc: 0.5132\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.3327 - acc: 0.5213 - val_loss: 1.3797 - val_acc: 0.5039\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.3373 - acc: 0.5205 - val_loss: 1.3715 - val_acc: 0.5045\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.3288 - acc: 0.5193 - val_loss: 1.3539 - val_acc: 0.5130\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.3222 - acc: 0.5241 - val_loss: 1.3619 - val_acc: 0.5113\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.3179 - acc: 0.5270 - val_loss: 1.3804 - val_acc: 0.5033\n",
            "Experiment with Regulizer = 0.000000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,742,474\n",
            "Trainable params: 1,740,682\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 17s 337us/step - loss: 2.3665 - acc: 0.2234 - val_loss: 1.8269 - val_acc: 0.3450\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.9734 - acc: 0.3062 - val_loss: 1.7310 - val_acc: 0.3731\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.8418 - acc: 0.3409 - val_loss: 1.6984 - val_acc: 0.3838\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 1.7774 - acc: 0.3625 - val_loss: 1.6185 - val_acc: 0.4217\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.7222 - acc: 0.3833 - val_loss: 1.5829 - val_acc: 0.4354\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.6917 - acc: 0.3914 - val_loss: 1.5644 - val_acc: 0.4403\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 16s 312us/step - loss: 1.6618 - acc: 0.4047 - val_loss: 1.5572 - val_acc: 0.4391\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.6356 - acc: 0.4131 - val_loss: 1.5360 - val_acc: 0.4475\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 14s 288us/step - loss: 1.6098 - acc: 0.4239 - val_loss: 1.4986 - val_acc: 0.4686\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.5916 - acc: 0.4314 - val_loss: 1.5007 - val_acc: 0.4654\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 15s 302us/step - loss: 1.5700 - acc: 0.4371 - val_loss: 1.4890 - val_acc: 0.4672\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 15s 297us/step - loss: 1.5595 - acc: 0.4418 - val_loss: 1.4826 - val_acc: 0.4703\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 15s 298us/step - loss: 1.5434 - acc: 0.4456 - val_loss: 1.4620 - val_acc: 0.4760\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 15s 297us/step - loss: 1.5348 - acc: 0.4513 - val_loss: 1.4480 - val_acc: 0.4812\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 15s 301us/step - loss: 1.5188 - acc: 0.4570 - val_loss: 1.4686 - val_acc: 0.4711\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 15s 295us/step - loss: 1.5100 - acc: 0.4582 - val_loss: 1.4448 - val_acc: 0.4774\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 15s 294us/step - loss: 1.5057 - acc: 0.4619 - val_loss: 1.4408 - val_acc: 0.4813\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 15s 308us/step - loss: 1.4952 - acc: 0.4621 - val_loss: 1.4464 - val_acc: 0.4812\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 15s 297us/step - loss: 1.4792 - acc: 0.4698 - val_loss: 1.4599 - val_acc: 0.4795\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 15s 305us/step - loss: 1.4735 - acc: 0.4695 - val_loss: 1.4258 - val_acc: 0.4890\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 1.4634 - acc: 0.4749 - val_loss: 1.3989 - val_acc: 0.4975\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 15s 291us/step - loss: 1.4471 - acc: 0.4797 - val_loss: 1.4180 - val_acc: 0.4901\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 15s 306us/step - loss: 1.4458 - acc: 0.4812 - val_loss: 1.4189 - val_acc: 0.4886\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 310us/step - loss: 1.4459 - acc: 0.4803 - val_loss: 1.4087 - val_acc: 0.4959\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 14s 285us/step - loss: 1.4362 - acc: 0.4843 - val_loss: 1.3937 - val_acc: 0.5009\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 14s 279us/step - loss: 1.4208 - acc: 0.4891 - val_loss: 1.4229 - val_acc: 0.4860\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 15s 294us/step - loss: 1.4155 - acc: 0.4881 - val_loss: 1.3922 - val_acc: 0.4980\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 311us/step - loss: 1.4030 - acc: 0.4965 - val_loss: 1.3971 - val_acc: 0.5006\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 1.3990 - acc: 0.4949 - val_loss: 1.4124 - val_acc: 0.4919\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 15s 301us/step - loss: 1.3962 - acc: 0.4971 - val_loss: 1.4179 - val_acc: 0.4935\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 15s 301us/step - loss: 1.3939 - acc: 0.4996 - val_loss: 1.3647 - val_acc: 0.5076\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 15s 305us/step - loss: 1.3873 - acc: 0.4997 - val_loss: 1.3858 - val_acc: 0.5038\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.3868 - acc: 0.5026 - val_loss: 1.3626 - val_acc: 0.5068\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.3747 - acc: 0.5031 - val_loss: 1.3902 - val_acc: 0.5013\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.3649 - acc: 0.5070 - val_loss: 1.3969 - val_acc: 0.4998\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 315us/step - loss: 1.3644 - acc: 0.5058 - val_loss: 1.3635 - val_acc: 0.5117\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.3508 - acc: 0.5167 - val_loss: 1.3575 - val_acc: 0.5108\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 1.3559 - acc: 0.5133 - val_loss: 1.3592 - val_acc: 0.5127\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 314us/step - loss: 1.3489 - acc: 0.5153 - val_loss: 1.3571 - val_acc: 0.5175\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 16s 314us/step - loss: 1.3360 - acc: 0.5209 - val_loss: 1.3682 - val_acc: 0.5117\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.3314 - acc: 0.5192 - val_loss: 1.3805 - val_acc: 0.5095\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.3363 - acc: 0.5155 - val_loss: 1.3780 - val_acc: 0.5072\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 16s 314us/step - loss: 1.3226 - acc: 0.5225 - val_loss: 1.3349 - val_acc: 0.5221\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.3250 - acc: 0.5263 - val_loss: 1.4072 - val_acc: 0.5008\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 1.3256 - acc: 0.5229 - val_loss: 1.3635 - val_acc: 0.5131\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 1.3185 - acc: 0.5259 - val_loss: 1.3719 - val_acc: 0.5063\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 15s 307us/step - loss: 1.3113 - acc: 0.5283 - val_loss: 1.3631 - val_acc: 0.5145\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.3076 - acc: 0.5312 - val_loss: 1.3465 - val_acc: 0.5218\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 316us/step - loss: 1.3049 - acc: 0.5296 - val_loss: 1.3554 - val_acc: 0.5211\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 1.3063 - acc: 0.5312 - val_loss: 1.3691 - val_acc: 0.5101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P9_1jgaFO9P",
        "colab_type": "code",
        "outputId": "df218aad-58f5-4410-8563-6be02b4f1d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\"\"\"Code Here\n",
        "將結果繪出\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
        "plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n",
        "plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xUVf7/8dcnZdJ7QoAQCKGGElpA\nEJSAigU7tl1Ql3Vt6+r6dXe/uH6/6rrlu/pbV10ri4pdFMG+KqICQWmG3nsLLT2kZzJzfn/cARGT\nkD6Zmc/z8cgjycydez83hPecnHvOuWKMQSmllOfzc3cBSimlWocGulJKeQkNdKWU8hIa6Eop5SU0\n0JVSyktooCullJfQQFdKKS+hga68nojsE5Hz3V2HUm1NA10ppbyEBrryWSJyq4jsEpFCEflYRLq6\nHhcReVJEckXkuIhsFJFBrucuEZEtIlIqIodE5PfuPQulfqCBrnySiEwE/g5cB3QB9gPvuJ6eBJwL\n9AWiXNsUuJ57GbjdGBMBDAK+aceylWpQgLsLUMpNpgKzjTFrAETkj0CRiKQAdiAC6A+sMsZsPeV1\ndmCAiKw3xhQBRe1atVIN0Ba68lVdsVrlABhjyrBa4UnGmG+AZ4HngFwRmSUika5NpwCXAPtFZImI\njGnnupWqlwa68lWHgR4nvhGRMCAOOARgjHnaGDMCGIDV9fIH1+PfG2OuADoBHwJz27lupeqlga58\nRaCIBJ/4AOYA00VkqIgEAf8HrDTG7BORkSJylogEAuVAFeAUEZuITBWRKGOMHTgOON12RkqdRgNd\n+YrPgMpTPjKBB4H5wBGgF3CDa9tI4EWs/vH9WF0x/3A9dyOwT0SOA3dg9cUr1SGI3uBCKaW8g7bQ\nlVLKS2igK6WUl9BAV0opL6GBrpRSXsJtM0Xj4+NNSkqKuw6vlFIeafXq1fnGmIS6njtjoItIMvA6\nkAgYYJYx5l/1bDsSWA7cYIyZ19B+U1JSyM7OPtPhlVJKnUJE9tf3XGNa6LXA74wxa0QkAlgtIguN\nMVtOO4g/8BjwZYuqVUop1Sxn7EM3xhw5sYCRMaYU2Aok1bHp3ViTNHJbtUKllFKN0qSLoq6V6IYB\nK097PAm4CnjhDK+/TUSyRSQ7Ly+vaZUqpZRqUKMviopIOFYL/F5jzPHTnn4KmGGMcYpIvfswxswC\nZgFkZGToFFWlvJDdbicnJ4eqqip3l+LRgoOD6datG4GBgY1+TaMC3bVI0XzgLWPM+3VskgG84wrz\neOASEak1xnzY6EqUUl4hJyeHiIgIUlJSaKiBp+pnjKGgoICcnBx69uzZ6Nc1ZpSLYN2lZasx5ol6\nDt7zlO1fBT7VMFfKN1VVVWmYt5CIEBcXR1O7phvTQh+LtcLcRhFZ53rsAaA7gDFmZpOOqJTyehrm\nLdecn+EZA90Y8y3Q6D0bY37R5CqaYPvRUj5ef4hfjUslJszWlodSSimP4nFT//cVlPPcot0cKq50\ndylKqQ6ouLiY559/vlmvveSSSyguLm709n/60594/PHHm3WstuBxgR4fbrXKC8pr3FyJUqojaijQ\na2trG3ztZ599RnR0dFuU1S48LtBjw4IAKCyvdnMlSqmO6P7772f37t0MHTqUP/zhDyxevJhzzjmH\nyy+/nAEDBgBw5ZVXMmLECAYOHMisWbNOvjYlJYX8/Hz27dtHWloat956KwMHDmTSpElUVjbcK7Bu\n3TpGjx5Neno6V111FUVFRQA8/fTTDBgwgPT0dG64wbop1pIlSxg6dChDhw5l2LBhlJaWtsq5u21x\nruaKO9FCL9MWulId3SOfbGbL4dOnrbTMgK6RPHzZwHqff/TRR9m0aRPr1lljOBYvXsyaNWvYtGnT\nySGAs2fPJjY2lsrKSkaOHMmUKVOIi4v70X527tzJnDlzePHFF7nuuuuYP38+06ZNq/e4N910E888\n8wzjx4/noYce4pFHHuGpp57i0UcfZe/evQQFBZ3sznn88cd57rnnGDt2LGVlZQQHB7f0xwJ4YAs9\nIigAm78f+RroSqlGGjVq1I/Gcz/99NMMGTKE0aNHc/DgQXbu3PmT1/Ts2ZOhQ4cCMGLECPbt21fv\n/ktKSiguLmb8+PEA3HzzzWRlZQGQnp7O1KlTefPNNwkIsNrQY8eO5b777uPpp5+muLj45OMt5XEt\ndBEhNsymXS5KeYCGWtLtKSws7OTXixcv5quvvmL58uWEhoaSmZlZ56zWoKCgk1/7+/ufsculPv/5\nz3/Iysrik08+4W9/+xsbN27k/vvvZ/LkyXz22WeMHTuWBQsW0L9//2bt/1Qe10IHq9tFu1yUUnWJ\niIhosE+6pKSEmJgYQkND2bZtGytWrGjxMaOiooiJiWHp0qUAvPHGG4wfPx6n08nBgweZMGECjz32\nGCUlJZSVlbF7924GDx7MjBkzGDlyJNu2bWtxDeCBLXSAuPAg8nWUi1KqDnFxcYwdO5ZBgwZx8cUX\nM3ny5B89f9FFFzFz5kzS0tLo168fo0ePbpXjvvbaa9xxxx1UVFSQmprKK6+8gsPhYNq0aZSUlGCM\n4Z577iE6OpoHH3yQRYsW4efnx8CBA7n44otbpQYxxj1rZGVkZJjm3uDivnfXsWpfId/OmNjKVSml\nWmrr1q2kpaW5uwyvUNfPUkRWG2My6treI7tcrD50baErpdSpPDLQ48KDqKhxUFHT8CQBpZTyJR4a\n6DoWXSmlTueZgR6m0/+VUup0nhno4Tr9XymlTueZge5qoetsUaWU+oFnBrr2oSulWlF4eDgAhw8f\n5pprrqlzm8zMTOoaal3f4+7gkYEeagsgJNBfu1yUUq2qa9euzJs3z91lNJtHBjro9H+lVN3uv/9+\nnnvuuZPfn7gJRVlZGeeddx7Dhw9n8ODBfPTRRz957b59+xg0aBAAlZWV3HDDDaSlpXHVVVc1ai2X\nOXPmMHjwYAYNGsSMGTMAcDgc/OIXv2DQoEEMHjyYJ598Eqh7Wd2W8sip/6DT/5XyCJ/fD0c3tu4+\nOw+Gix+t9+nrr7+ee++9l7vuuguAuXPnsmDBAoKDg/nggw+IjIwkPz+f0aNHc/nll9d7784XXniB\n0NBQtm7dyoYNGxg+fHiDZR0+fJgZM2awevVqYmJimDRpEh9++CHJyckcOnSITZs2AZxcQreuZXVb\nynNb6GE2Csq0y0Up9WPDhg0jNzeXw4cPs379emJiYkhOTsYYwwMPPEB6ejrnn38+hw4d4tixY/Xu\nJysr6+T65+np6aSnpzd43O+//57MzEwSEhIICAhg6tSpZGVlkZqayp49e7j77rv54osviIyMPLnP\n05fVbSnPbaGH2dh6pHUXzldKtbIGWtJt6dprr2XevHkcPXqU66+/HoC33nqLvLw8Vq9eTWBgICkp\nKXUum9vaYmJiWL9+PQsWLGDmzJnMnTuX2bNn17msbkuD3XNb6OFBFJTV4K7FxZRSHdf111/PO++8\nw7x587j22msBa9ncTp06ERgYyKJFi9i/f3+D+zj33HN5++23Adi0aRMbNmxocPtRo0axZMkS8vPz\ncTgczJkzh/Hjx5Ofn4/T6WTKlCn89a9/Zc2aNfUuq9tSHttCjw+3UeNwUlpdS2RwoLvLUUp1IAMH\nDqS0tJSkpCS6dOkCwNSpU7nssssYPHgwGRkZZ7yhxJ133sn06dNJS0sjLS2NESNGNLh9ly5dePTR\nR5kwYQLGGCZPnswVV1zB+vXrmT59Ok6nE4C///3v9S6r21IeuXwuwPtrcrhv7noW/z6TlPiwM79A\nKdUudPnc1uMTy+fCD9P/C3QsulJKAZ4c6Dr9XymlfsRjAz3+RAtdA12pDkcHK7Rcc36GHhvoMWHW\nhVCd/q9UxxIcHExBQYGGegsYYygoKCA4OLhJrzvjKBcRSQZeBxIBA8wyxvzrtG2mAjMAAUqBO40x\n65tUSRMFBfgTERygXS5KdTDdunUjJyeHvLw8d5fi0YKDg+nWrVuTXtOYYYu1wO+MMWtEJAJYLSIL\njTFbTtlmLzDeGFMkIhcDs4CzmlRJM8SHB+lNLpTqYAIDA+nZs6e7y/BJZwx0Y8wR4Ijr61IR2Qok\nAVtO2WbZKS9ZATTtbaWZYnX6v1JKndSkPnQRSQGGASsb2OwW4PN6Xn+biGSLSHZr/DkWF2ajUFvo\nSikFNCHQRSQcmA/ca4ypcxEVEZmAFegz6nreGDPLGJNhjMlISEhoTr0/EhcepH3oSinl0qip/yIS\niBXmbxlj3q9nm3TgJeBiY0xB65VYv/hwG0UVNTidBj+/upfAVEopX3HGFrpYiwW/DGw1xjxRzzbd\ngfeBG40xO1q3xPrFhtlwOA0llfb2OqRSSnVYjWmhjwVuBDaKyDrXYw8A3QGMMTOBh4A44HnXYvG1\n9a010JpOnf4f45o5qpRSvqoxo1y+xRpf3tA2vwJ+1VpFNVb8KdP/e3dq76MrpVTH4rEzRQFiw61A\n1+n/Sinl4YEeF2Z1uej0f6WU8vBAjwkNRERXXFRKKfDwQA/w9yMm1KZroiulFB4e6HBi+r+20JVS\nyuMDPS7Mpgt0KaUUXhDo8eFBukCXUkrhBYEeqy10pZQCvCDQ48JtFFfYqXU43V2KUkq5lRcEumss\neoW20pVSvs3jA/3E9H8d6aKU8nUeH+ixGuhKKQV4QaCfuuKiUkr5Mo8P9HhdoEsppQAvCPTI4EAC\n/ERb6Eopn+fxge7nJ8TozaKVUsrzAx2s6f+64qJSytd5RaDr9H+llPKSQNfp/0op5SWBHhduo1C7\nXJRSPs4rAj0+PIjS6lqq7A53l6KUUm7jFYEe55otqiNdlFK+zCsCXaf/K6WUlwS6Tv9XSikvCXSd\n/q+UUl4S6Ce7XLSFrpTyYV4R6OFBAdgC/HQsulLKp3lFoIsI8WE27XJRSvm0Mwa6iCSLyCIR2SIi\nm0Xkt3VsIyLytIjsEpENIjK8bcqtX5xO/1dK+biARmxTC/zOGLNGRCKA1SKy0Biz5ZRtLgb6uD7O\nAl5wfW43Ov1fKeXrzthCN8YcMcascX1dCmwFkk7b7ArgdWNZAUSLSJdWr7YBceHa5aKU8m1N6kMX\nkRRgGLDytKeSgIOnfJ/DT0MfEblNRLJFJDsvL69plZ5BfHgQBeXVGGNadb9KKeUpGh3oIhIOzAfu\nNcYcb87BjDGzjDEZxpiMhISE5uyiXrFhNqrsTipqdD0XpZRvalSgi0ggVpi/ZYx5v45NDgHJp3zf\nzfVYu4nT6f9KKR/XmFEuArwMbDXGPFHPZh8DN7lGu4wGSowxR1qxzjOK1+n/Sikf15hRLmOBG4GN\nIrLO9dgDQHcAY8xM4DPgEmAXUAFMb/1SGxan0/+VUj7ujIFujPkWkDNsY4C7Wquo5ugUEQzAwaIK\nd5ahlFJu4xUzRQESI4NIig5h1d5Cd5eilFJu4TWBLiKMTo1jxZ4CnE4duqiU8j1eE+gAY3rFUVRh\nZ/uxUneXopRS7c7rAh1g+e4CN1eilFLtz6sCPSk6hO6xoSzfo4GulPI9XhXoAKNTY1m1t1D70ZVS\nPsfrAn1MrzhKKu1sOdKs1QmUUspjeV+gp8YDsEK7XZRSPsbrAr1zVDA948P0wqhSyud4XaADjE6N\nY9XeQmodTneXopRS7cYrA31MrzhKq2vZfFj70ZVSvsMrA310aiyADl9USvkUrwz0ThHB9ErQfnSl\nlG/xykAHq9sle18hdu1HV0r5CO8N9NR4ymscbDxU4u5SlFKqXXhtoJ/sR9duF6WUj/DaQI8LD6Jf\nYoROMFJK+QyvDXQ40Y9eRE2t9qMrpbyfVwf66NQ4Ku0O1ucUu7sUpZRqc14e6LGIaD+6Uso3eHWg\nR4fa6N85UgNdKeUTvDrQAcakxrH6QBFVdoe7S1FKqTbl/YHeK46aWifrDmo/ulLKu3l9oI/qGYuf\nwLc7891dilJKtSmvD/SokEDO6ZPAWyv3U1Zd6+5ylFKqzXhmoJflNmnz+y7oS1GFndnf7m2jgpRS\nyv08L9A3zIXH+0Lhnka/ZEhyNJMGJPJi1h6KK2rasDillHIfzwv0HmcDBjbOb9LL7pvUl7KaWmZl\nNf6NQCmlPInnBXpUN+h+NmyaB8Y0+mX9O0dyWXpXXvluH3ml1W1YoFJKuccZA11EZotIrohsquf5\nKBH5RETWi8hmEZne+mWeZvAUyNsGxzY36WX3nt+HGoeTFxbvbqPClFLKfRrTQn8VuKiB5+8Cthhj\nhgCZwD9FxNby0how4EoQf6uV3gSpCeFMGZ7Emyv3c6Skso2KU0op9zhjoBtjsoDChjYBIkREgHDX\ntm07PjAsHnpNgE3zm9TtAnD3xD4YY3jmm11tVJxSSrlHa/ShPwukAYeBjcBvjTF1rlcrIreJSLaI\nZOfl5bXsqIOugeIDkPN9k16WHBvKz0Z1Z+73BzlQUNGyGpRSqgNpjUC/EFgHdAWGAs+KSGRdGxpj\nZhljMowxGQkJCS07av/JEBAMG5vW7QJw14Te+PsJT329o2U1KKVUB9IagT4deN9YdgF7gf6tsN+G\nBUdCn0mw+QNwNK2HJzEymJvPTuHDtYfYlVvaRgUqpVT7ao1APwCcByAiiUA/oH0Gew++BspzYd/S\nJr/0jvG9CAn05/EF2kpXSnmHxgxbnAMsB/qJSI6I3CIid4jIHa5N/gKcLSIbga+BGcaY9lkJq88k\nsEU0q9slNszG7eN78cXmo7pwl1LKK4hp4iiR1pKRkWGys7NbvqMP7oBtn8EfdkJAUJNeWmV3cOFT\nWfiL8Pm95xAU4N/yepRSqg2JyGpjTEZdz3neTNHTDboGqktg58ImvzQ40J9HLh/InvxyXlqqC3cp\npTyb5wd66ngIjW/yJKMTMvt14qKBnXnmm50cLNRhjEopz+X5ge4fCAOvhO1fQHVZs3bx0GUDEIQ/\nf7qllYtTSqn24/mBDla3S20lbP+sWS/vGh3CPef1YeGWY3yz7VgrF6eUUu3DOwI9+SyI7Nas0S4n\n3DKuJ70Swnj44816Q2mllEfyjkD384NBV8Pur6GioWVn6mcL8OMvVwziYGElz+tqjEopD+QdgQ4w\n+Fpw1sIXfwRn81rYZ/eO57IhXZm5ZDf78stbuUCllGpb3hPoXdIh8wHY8I41Nr2JywGc8L+T07D5\n+/HgR5twON0zRl8ppZrDewIdIHMGTHwQNs6F928Fh73Ju0iMDGbGRf1YujOf37y9hupa7U9XSnmG\nAHcX0OrO/T3422Dhg+C0w5TZENC0+23cOCaF6lonf/3PVkpe+Z5ZN2UQHuR9PyqllHfxrhb6CWPv\ngQv/Dls/gfduhtqm30P0V+ek8sR1Q1i5t5CfzVpBfpneh1Qp1bF5Z6ADjPk1XPK4NTb93Wlgr2ry\nLq4e3o0XbxrBztxSrp25XGeSKqU6NO8NdIBRt8KlT8HOL60umGaY2D+RN285i4Kyaqa8sIxtR4+3\ncpFKKdU6vDvQATKmw9CpsPYtqCpp3i5SYnnvjrMRgetmLuf7fc0b666UUm3J+wMdYOSvwF4O699p\n9i76dY5g3h1nEx8exLSXVvLFpqOtWKBSSrWcbwR60nDoOhy+fwlasP57cmwo8+48m7Qukfz6rdW8\nuWJ/KxaplFIt4xuBDlZ/ev4O2LukRbuJDbPx9q1nkdmvE//74Sb++eV23HWTEKWUOpXvBPrAqyEk\nFla92OJdhdoCmHXjCK7L6MYz3+xixvwN1DqcrVCkUko1n+8EemAwDL/RGsZYcqjFuwvw9+OxKenc\nM7E3c7NzuO2N1VTUNG+5AaWUag2+E+gAGb+0+tBXv9IquxMR7pvUj79dNYjF23O5+vlluqiXUspt\nfCvQY1Kg74Ww+jWorWm13U49qwevTB/F0eNVXPbst3y9VW+SoZRqf74V6AAjb4XyXNj6cavudnzf\nBD75zTi6x4Zyy2vZPLFwB05drVEp1Y58L9B7TYSYnq1ycfR0ybGhzL/zbK4Z0Y2nv97JL1/7nuKK\n1vtLQCmlGuJ7ge7nByNvgYMr4OjGVt99cKA//7gmnb9eOYjvduVz2bPfsvlw82aoKqVUU/heoIO1\nFEBAsDXRqA2ICNNG9+Dd28dgrzVc/fwy3ss+2CbHUkqpE3wz0ENjYfA1sGEuVBa32WGGd4/h03vG\nMaJHDH+Yt4E/vr9Bb0CtlGozvhnoYF0ctVfA+jltepj48CDeuOUs7prQizmrDnLNzGW6DK9Sqk34\nbqB3HQrdRkHW41Cwu00P5e8n/OHC/rx0Uwb7Cyq49JlvWbQtt02PqZTyPWcMdBGZLSK5IrKpgW0y\nRWSdiGwWkZYtltKernjO+vz6FVCS0+aHO39AIp/ePY6k6BCmv/o9f/9sK2XVOrtUKdU6GtNCfxW4\nqL4nRSQaeB643BgzELi2dUprBwl94cb3rXXSX78SyvLa/JA94sJ4/9dn87NRyfw7aw+Z/1jEG8v3\nYde1YJRSLXTGQDfGZAEN3dHh58D7xpgDru09qy+hyxD4+Vyrhf7mVW16kfSE4EB//n51Oh/dNZZe\nCeE8+NFmJj2Zxecbj+jKjUqpZmuNPvS+QIyILBaR1SJyU30bishtIpItItl5eW3fGm60HmPghjch\ndxu8fT3UtM96LEOSo3nnttG8fHMGAX7CnW+tYcoLy8jWOyIppZqhNQI9ABgBTAYuBB4Ukb51bWiM\nmWWMyTDGZCQkJLTCoVtR7/NhykuQs8q6qXRtdbscVkQ4Ly2Rz397Do9NGcyh4kqumbmc++dvoKTS\n3i41KKW8Q2sEeg6wwBhTbozJB7KAIa2w3/Y38Eq4/BnY/Q289wsoz2+3Qwf4+3H9yO4s+n0mt49P\nZW72QS54YgkLNuut7pRSjdMagf4RME5EAkQkFDgL2NoK+3WPYdPg4n/AjgXwr6Gw9AmwV7bb4UNt\nAfzx4jQ+vGsssWE2bn9jNXe9tYa80vb5i0Ep5bnkTBfhRGQOkAnEA8eAh4FAAGPMTNc2fwCmA07g\nJWPMU2c6cEZGhsnOzm5J7W0rbwd89bB1Q4zIbnDeQzD4WmstmHZidzj595LdPP31LkJs/jx46QCu\nHNqVAH/fnT6glK8TkdXGmIw6n3PXqIoOH+gn7F0KX/4PHFlvjYi54M+Qcm67Bvuu3FJmzN/I6v1F\nxIQGcl5aIhcO7Mw5feIJDvRvtzqUUu6ngd5STidsfA++/jMczwFbBCQNg6QM6JZhfY5IbOMSDF9u\nOcYXm47w9bZcSqtqCQn0Z3zfBCYNtAI+LCigTWtQSrmfBnprsVfClo/g4Co4lA3HNoPTNdMzqjuM\nvgPOugP82rbVXFPrZOXeAr7cfIwvtxzl2PFqYkIDuWVcT24ck0JUSGCbHl8p5T4a6G3FXml1xeRk\nw84FsDcLug6zRsp0HtwuJTidhuz9Rfx7yW6+3pZLRFAAN5+dwi/H9SQ2zNYuNSil2o8GenswBja/\nD5/PgIpCOPtuyLwfAkParYRNh0p4fvEuPt90lOAAf6aN7s6No1PoHhfabjUopdqWBnp7qiiEhQ/C\n2jetW91d9hSkZrZrCTuPlfL84t18tO4QTgPdYkIYkxrH2b3jGJMaT+eo4HatRynVejTQ3WFvFnzy\nWyjcA2PvhfP/BCLtWsLBwgq+3nqM5XsKWLGn8OTM09T4MCb078Qvx/UkKbr9/oJQSrWcBrq72Cut\nLpg1r8Gkv1rdMG7idBq2HDnOij0FLNtdwNKdeRgDVw1L4s7MXqQmhLutNqVU42mgu5PTCfN/CZs/\ngKtfgvSOsbrw4eJKZmXtYc6qA9gdTiand+WuCb3o3znS3aUppRqgge5utdXw5hQ4sAKmzWv3PvWG\n5JVW89K3e3hz+X7KaxxM7N+J89MSGdc7Xi+mKtUBaaB3BJXF8MolUHwApn8GXdLdXdGPFFfU8Oqy\nfbyz6iBHj1cBkBwbwthe8YztHc/ZveKICw9yc5VKKQ30juL4YXjpAnDa4ZaFENPD3RX9hDGG3Xnl\nLNudz7c781m+p4DSKmvyVFyYjW6xoSTHhJAcG0r32FCSY0IZ3C1KJzMp1U400DuS3K0w+0II6wS3\nfAnB0VB93LoN3okPP3/rBtb+7p/KX+twsunwcVbuKWBfQQUHCys4WFTBoaJKap3W706AnzCmVxyT\nBiRywYDOOixSqTakgd7R7F9u3ZgawFED1PFvEBoPA6+yVnhMHtXuQx7PxOE0HD1exf78cpbszOPL\nzcfYm2/d6WlItygmDezMZeldtR9eqVamgd4R7fsWtn4KwZEQHPXjj8pia9bp9s+htspaJ2bwFBg0\nBToNbNeVHhvL6qopY8HmY3y55RjrDxbjJ3BpelfumtCbfp0j3F2iUl5BA91TVZfCtv9YKz3uXgTG\nYa302HmwtZRvl3TonA4J/cC/Y/VhHy6u5LVl+3hzhTV65oIBifxmQm+GJEe7uzSlPJoGujcoz7da\n7EfWwZENcGwT2Cus5/yDYNhU6yYcITHurfM0xRU1vPLdPl5dto+SSjvn9Iln+tgU0rtFE6+jZpRq\nMg10b+R0QMEuK9z3LYW1b0BoHEz6G6Rf1+H63Muqa3lrxX5eXLqX/DLrdnpxYTb6JkbQr3MEfRMj\nSOsSweCkKL0jk1IN0ED3BUfWw6f3Weu0p5wDlz4J8X2ato+aCms/3Ue32RtCld3B6v1FbD9ayo5j\npWw/VsqOo6WU1zgAiA4NJLNvAuelJTK+XwKRwR2rK0kpd9NA9xVOJ6x5Fb76k7WOzNjfwjm/a9wS\nvjsWwGe/tyY+9bsELn8WwuLaumLAWmfmUHElG3JK+HrbMRZty6Wowk6AnzCqZyznpSVy1bAkXd9d\nKTTQfU9ZLnz5v7DhXQhLgBG/gBHTISrpp9uWHIIvZsDWTyChvxXmy5+1+uKvmgm9JrZiXXmw7i0o\nz7P6+wPq7kN3OA1rDxTx1dZcvt56jJ25ZYTa/LlxdA9+dU4qCRHa9658lwa6r9r3HSx7BnZ8AeIH\n/SfDqFutLhmnA1b9Gxb9n/X1+P+GMb+BAJvVLz//V5C/3XqsgfA9I2OsPv7sV6w3Dae1hC/9JsN1\nrzVqdM72o6U8v3gXn6w/jC3Aj5+N6s4d43uRGKkTmJTv0UD3dUX7IHs2rHkdKouslrhfgDVSps8k\nuOQfEJPy49fUVFit/OyXrUVJ6/wAABJ8SURBVGGSU2ZDQt/GHc8YKDkIWz6G1a9CwU5rfP3QqdZf\nC3uzrO6dAVdY+23kjNg9eWU8v3g3H6w9hL8I149MJrNfAiGB/gTb/AkJtD5Cbf4E+Ptx4nfbuEoC\nCLH5E64301YeTANdWeyVsOl9WDXLWm7g/Ecg7bKGL4Bu+ww+/g3UlEPiIIjqBtHJEJVsfR3ZFcoL\nIG8b5G2F3G2Qtx1qSq3XJ59ldfcMvPLHffnLn4MFD1iTpa5+sUk31j5QUMELS3Yxb3UOdkfTfn/9\nBIYmRzOxfycy+3ViYNdIpIONCFKqIRroqmVKj0LW41ZLuyTH+qit+ul2YQlW679TmvW5+xhIHFD/\nfr99Cr56GNJvgCufb1KoA+SXVXO4uJLKGgcVdgdVNQ4q7daHvdaJiJx8rxIAEXKPV7FkRx4bckoA\n6BQRxIR+nZjQvxPnpXUiUIdMqg5OA121LmOgosDqVik5ZF1ATejfvFExWf+Ab/4Kw6bBZc+027IG\neaXVLN6ey+LteWTtyKO0upauUcH8clxPbhjVXbtlVIelga46tkX/B0ses7pmJj/R7mvV2B1Osnbk\nMStrDyv3FhIZHMBNY1K4+ewUHVGjOpyGAl2bIcr9Mv8IDjt8+4TVvXP1LGvRsqYyxro7lKMaAkMb\nvb5NoL8f56Ulcl5aImsPFPHvJXt4bvEuPl66ipmx71CbdhXhGTfQPTZUZ7GqDk1b6KpjMAZWvQhf\n3A9xveCGORDfu/7t9y+Drx6x+vVra6wQd9T88LwtHHqfZw2P7HMBhMY2qZycrSuJmP9zomrzAXjE\nfiNvMZmU+FB6dwqnd0I4o3rGMaZXHP5+TbioWlkM+TtcF5G3Wx/HD8OVz0HXYU2qUfmmFnW5iMhs\n4FIg1xgzqIHtRgLLgRuMMfPOVJQGuqrT3qXw3s1Wi33KS9D3wh8/X7QfFj4EWz6EyCTr+YBg8LdZ\nY+UDgqzFygp2WouZlR0D8YceZ0O/i6H/pWe+U9Sur2HuzRAcScXVr1Gz6HGi93/Bd12n84rt5+zK\nK+dAYQVOA12jgrlqeBJThncjNSG87v2V5MCKF2DjPCg7+sPjAcHW8gwlh6xzuW1Rh1s1U3U8LQ30\nc4Ey4PX6Al1E/IGFQBUwWwNdtUjxAXhnKhzdCBP/11q+oKbc6pJZ9qw1SWrcvXD2PWBr4AYaTicc\nXgvb/2MNv8zbCog1G/bs31ijcE4fsrj2Tfjkt9ZF3qnvWcMynQ749F5rHP+I6TD5n1TWwldbjzF/\nTQ5ZO/JwGhjePZopI7oxaUBn4sNtSO4W+O5p2DTP+gsk7VJIGmHtO74vRHe3RvZs+Rjm3ggX/AXG\n3tOmP1qfUrgXPr4bJv3Fq/76afFFURFJAT5tINDvBezASNd2GuiqZWoqrP+Mm+ZZyw8c22K1bgdf\nC+f/yRoD31SFe2Dd2/D9y1BZCF2HW8GedoUVrIsfhSWPQuoEuO71H/fjGwNf/9l6UxlwhTV23jV7\n9tjxKj5ce4h5q3PYmVvKaL+t/DrwU86VdVRJMNlxl7On901EJqYSF24jNsxGXFgQsWE2bAF+1r7f\n+TnsWQy/XtEh7zXrkT6+23oTjuoOty9pcrdbR9WmgS4iScDbwARgNg0EuojcBtwG0L179xH79+9v\n5Ckon2SMta7MwoesFtZFj1q342upmgpY/zYsfx4Kd1v/4Tv1h51fwtBpcNlT9Xd9LHsWvvwfSM2E\niQ9B8T4o2A0FuzGFu3Hk7SKguojygBgWRl7FO+YCtpcEUFRhr3N3EcEBxIXZ6BVUzHNFd7AndAjv\n9Pkn0aE2okJtpCaEMTgpqnXWjnc6XUsur4PD66y/XooPQOb9MPzGlu+/Iyk9Ck8NtlYO3b/cup5y\nw5wOebevpmrrQH8P+KcxZoWIvIq20FVrK8+HkNjW/8/odMKOz62QPrDMGm0zfsaZlw5eNwc+usu6\ng9QJkUkQm2pd0E0aYf0lccrM2PLqWo6UVFFYXkNheTUF5TUUlNVQWF5DQXkNJZV2zi2Yy6/KX+S/\nuZf3qkdx6n/NrlHBDEqKYnBSFIO6RZEUHYK/nxDgJ67Pfvj7CRHBAQQHnjZB68BK+OYvVoifmMEb\nEGwt6eCstYL9sqdhxM0t/IF2IAsfhmVPw92rYedC+Py/rb/sxv2XuytrsbYO9L24JuIB8UAFcJsx\n5sOG9qmBrjqUymIIacLt8Q6vs1q3cb0gpmfDffmN5XTAixPh+GEcd62ixISx/Wgpmw6VsPFQCZsO\nlWAKdvFwwOtUYuMh+3Ty+HHNQQF+TBrYmauGdeWcPgkEbphj9f+HJ0Lfi6DrUOgy1OrH9w8AexW8\nOxV2feU9oV51HJ4caLXKr33V+ktv3nTY8hHc/AmkjHN3hS3S5n3op2z3KtpCV6r5Dq+DFyfA8Jut\nrp8Tamtg2b8wS/6Bwy8QcdipDQhl7ZA/cSDxPBxOQ63TsONoKZ9uOExJRTV/DnmXaeYTSruOJXza\nm0h9fcgdJdRLDsHGudZooKoSq887JNa6E1eo63PPc60RSw357mlY+CDctviHi6HVpTBrgrWG0e1L\nISKxrc+mzbR0lMscIBOr9X0MeBgIBDDGzDxt21fRQFeqZb54AFY8B79cYPUBH1wFH99jjdIZcCVc\n/JgVeO/fat1hauhU6/qC6yJuTVkRx9+8kfijS3ndeRF/rvk53eOjOH9AIhP6dSIjJeana9bYq+Dd\nabBrYfuGek05bP3UuqaxZwlgrAXdYlOhotBaYqLS9bmqBPwCYfrnkDyy7v3VVsO/hlijiG7++MfP\nHdti/QXULQNu/PCnq3zW1ljXF/xtVndUE9cWahJjmn1XMJ36r5QnqS6D50eDLczqHvj+ZauPfvLj\n1lj6E2prIOv/wdJ/QmQ364YkEZ3h7euhaC9c8jglA6fx+cYjfLrhCCv3FmB3GCKCAzi3TwIT+nci\ns18CcWE2a8XJ9gr12hrYu8Ra+XPLR2Avt4ZwDvkZpF9vdWPVpaIQZmVa/f63Z0FY/E+3WfumdX1j\n2vtWl8vp1s2BD++whsJm/tG6frA3C/Z9CwdX/nDj9ZAY674BqeOtUU+xqS2/LaPDDts+tf49B10N\nGb9s1m400JXyNNu/gDnXAwJn3QET/weCIure9uAqeP82a917W5jVwrz+jZ/0FZdV1/Ltzny+2XaM\nRdvzyCutPvmcv+viaqifnWf8nuAc1rIqdDyxfUbSc8BZ+HcZbL1ZNDfUHHarBb7lA6tFXlUMQVEw\n8AoryJNHN+6i9+F18PIk6DHGCu1TW9FOp/VG6G+DO5bWX+uJ4YyBYdabCUCngdDzHOgx1lpJdM8S\naxjp8Rzr+chu1pvp2HusN5+mKMmB1a/BmtesiW5RydabybCpTduPiwa6Up5o/TtW10HS8DNvW11m\nDe/M3WK11E+/YclpnE7D5sPHWbY7n/IaBw6nk1qnwek0mNoqLtj/JN0Ll9GF/B9eExKHX+eB1jLJ\n1WVWv3RNKY6qUpxVpeB04BcSiX9ItHVDk+Ao60Kzw24NCa0sgqBIa2LXwKug14Tm3QlrzRvWGv3n\n/B7Oe/CHx7d9Bu/8DK5+CdKvrf/19kr49L+s5SFSxlkfdbX2jbHmLuxZZIX7jgXWY8OmWi38hoK9\nttpq+We/Yo2kMsZagiLjFutzC7pzNNCVUk1mdzj5Zu0OVq5YiuPIRgb6H+CskMNE+VVSZoIpdgRR\nYLdR7Aii3ARjx58IqaRTYBWdbdXE+lcQbsoJxIH0nugK8YnNv53hqT76Dax9A372LvS7yHrs5Qut\ndXHuWdvou2A1SUkOfPuk1bo/PdiNgWObfwj//cus7pvQeGuM/4hfnPFNtrE00JVSLbI7r4y3Vx5g\n3uocSirtxIcHkZoQRq+EcHq5PgcH+rPpUAnrcopZf7CYnKJKAAL8hPPTErl+ZDLn9k1o2mJm9bFX\nWl0vxfvhtiVWV8bsC+Hi/wdn3d7y/Tfk9GDveS4c3WDd/Bysv6pSM62+997ntc4b2Ck00JVSraK6\n1kGV3UlUyJkXEcsvq2ZDTjHf7Srgg7WHKCyvoXNkMFNGJHFdRjI94sJaVkzRPvj3uVYLOTwRDq2G\n/9psXUdoDyeCfdfX1siZ1AnWRdTmLEvRBBroSim3qql18vXWY7ybffDkYmajU2MZ0SOGHrFhdI8L\npUdcKIkRwfi5WvBVdgcHCivYm1/Ovvxy9hWUE2YL4PqRyfRJdF0g3rEA3r7O+nr8DJjwgJvOsP1o\noCulOowjJZXMX53Dh+sOsze/HIfzhwwKCvAjOTaUyhoHh0sqf7T8QVyYjdKqWmocTkb1jGXa6B5c\nNLAztu/+CdmzrZEtdV3c9DIa6EqpDsnucHK4uJL9BRUcKHR9FFQQHOhHSnwYPV0fPeLCiAoJpKCs\nmvdW5/DWyv0cLKwkPtzGdRnJXDIokWoHFFfUUFRhp7iihuIKOyWVduwOawRPreuzw/XRIy6UIcnR\nDE2OJik6xBqL7wE00JVSXsXpNGTtzOPNFQf4ZtsxnHXEmJ9AZEggQQF+JxcvC/C3FjQThL0F5dTU\nOgGIDw9iaHIUQ7pFMyTZ+mjMdQJ30HuKKqW8ip+fkNmvE5n9OnGouJLV+4uIDA4gOtRGTGgg0aE2\nIoICTvbH16Wm1sn2o6WsO1jEuoMlrM8p5qutuSefT00IY2hyNMOSoxmaHEO/zhHW+vUdmLbQlVLK\n5XiVnQ0HS1whX8y6g8Xkl1n3qhWBhPAgukSH0DUqmM5RwXSNCiExKpj4MBuxrpuXxIba6ryZuMNp\nqKippaLGQXCAP1GhzfsLQFvoSinVCJHBgYzrE8+4PtbFVWMMOUWVrDtYzM5jpRwpqeLo8Sp25paR\ntSOP8hpHnfuJCgkkJjQQu8MK8fIax8nuHYA7M3sx46L+rV6/BrpSStVDREiODSU59qfr3RtjKK2u\n5WhJ1cmblZx685LiSjs2fz9Cbf6EBvkTGhhAqM2fEJs/g5Oi2qReDXSllGoGESEyOJDI4EDoIMur\nd+wefqWUUo2mga6UUl5CA10ppbyEBrpSSnkJDXSllPISGuhKKeUlNNCVUspLaKArpZSXcNtaLiKS\nB+xv5svj4ZS71/oWXz13PW/fouddvx7GmIS6nnBboLeEiGTXtziNt/PVc9fz9i163s2jXS5KKeUl\nNNCVUspLeGqgz3J3AW7kq+eu5+1b9LybwSP70JVSSv2Up7bQlVJKnUYDXSmlvITHBbqIXCQi20Vk\nl4jc7+562oqIzBaRXBHZdMpjsSKyUER2uj7HuLPGtiAiySKySES2iMhmEfmt63GvPncRCRaRVSKy\n3nXej7ge7ykiK12/7++KiM3dtbYFEfEXkbUi8qnre68/bxHZJyIbRWSdiGS7HmvR77lHBbqI+APP\nARcDA4CficgA91bVZl4FLjrtsfuBr40xfYCvXd97m1rgd8aYAcBo4C7Xv7G3n3s1MNEYMwQYClwk\nIqOBx4AnjTG9gSLgFjfW2JZ+C2w95XtfOe8Jxpihp4w9b9HvuUcFOjAK2GWM2WOMqQHeAa5wc01t\nwhiTBRSe9vAVwGuur18DrmzXotqBMeaIMWaN6+tSrP/kSXj5uRtLmevbQNeHASYC81yPe915A4hI\nN2Ay8JLre8EHzrseLfo997RATwIOnvJ9jusxX5FojDni+vooHeZOhm1DRFKAYcBKfODcXd0O64Bc\nYCGwGyg2xtS6NvHW3/engP8GnK7v4/CN8zbAlyKyWkRucz3Wot9zvUm0hzLGGBHx2jGnIhIOzAfu\nNcYctxptFm89d2OMAxgqItHAB0B/N5fU5kTkUiDXGLNaRDLdXU87G2eMOSQinYCFIrLt1Ceb83vu\naS30Q0DyKd93cz3mK46JSBcA1+dcN9fTJkQkECvM3zLGvO962CfOHcAYUwwsAsYA0SJyouHljb/v\nY4HLRWQfVhfqROBfeP95Y4w55Pqci/UGPooW/p57WqB/D/RxXQG3ATcAH7u5pvb0MXCz6+ubgY/c\nWEubcPWfvgxsNcY8ccpTXn3uIpLgapkjIiHABVjXDxYB17g287rzNsb80RjTzRiTgvX/+RtjzFS8\n/LxFJExEIk58DUwCNtHC33OPmykqIpdg9bn5A7ONMX9zc0ltQkTmAJlYy2keAx4GPgTmAt2xlh6+\nzhhz+oVTjyYi44ClwEZ+6FN9AKsf3WvPXUTSsS6C+WM1tOYaY/4sIqlYLddYYC0wzRhT7b5K246r\ny+X3xphLvf28Xef3gevbAOBtY8zfRCSOFvyee1ygK6WUqpundbkopZSqhwa6Ukp5CQ10pZTyEhro\nSinlJTTQlVLKS2igK6WUl9BAV0opL/H/ARMUs3eS6SB6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5f4fOu4ZnQGb6qIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngKH1mojKTc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#second model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DmMoBXlAX1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#多了兩個num_neuroons\n",
        "\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "\"\"\"\n",
        "建立神經網路，並加入 L1 \n",
        "加入 dropout layer\n",
        "\n",
        "\"\"\"\n",
        "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128, 64, 32], l1_ratio=1e-4, drp_ratio=0.2):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    \n",
        "    for i, n_units in enumerate(num_neurons):\n",
        "        if i == 0:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1), \n",
        "                                   kernel_regularizer=l1(l1_ratio))(input_layer)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        else:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1),\n",
        "                                   kernel_regularizer=l2(l1_ratio))(x)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "            x = BatchNormalization()(x)\n",
        "    \n",
        "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
        "    \n",
        "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NoMLm10Ad-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "MOMENTUM = 0.95\n",
        "L1_EXP = [1e-2, 1e-4, 1e-8, 1e-12]\n",
        "\n",
        "Dropout_EXP = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHyXX67ZAt3O",
        "colab_type": "code",
        "outputId": "e2b9dc41-8752-4868-bea6-1d9443ab0880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = {}\n",
        "\"\"\"\n",
        "使用迴圈建立不同的帶不同 L1/L2 的模型並訓練\n",
        "\"\"\"\n",
        "for regulizer_ratio in L1_EXP:\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with Regulizer = %.6f\" % (regulizer_ratio))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:], drp_ratio=Dropout_EXP, l1_ratio=regulizer_ratio)\n",
        "    model.summary()\n",
        "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"exp-l1-%s\" % str(regulizer_ratio)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "Experiment with Regulizer = 0.010000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 1,752,234\n",
            "Trainable params: 1,750,250\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 150.0826 - acc: 0.1542 - val_loss: 16.6504 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 311us/step - loss: 10.6894 - acc: 0.1608 - val_loss: 9.3710 - val_acc: 0.1618\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 15s 304us/step - loss: 8.5641 - acc: 0.1739 - val_loss: 7.9217 - val_acc: 0.1793\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 7.4209 - acc: 0.1820 - val_loss: 6.8351 - val_acc: 0.1844\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 6.4902 - acc: 0.1860 - val_loss: 6.7382 - val_acc: 0.1482\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 5.7962 - acc: 0.1957 - val_loss: 6.4110 - val_acc: 0.1306\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 5.2707 - acc: 0.2017 - val_loss: 5.3152 - val_acc: 0.1604\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 4.8185 - acc: 0.2057 - val_loss: 5.5506 - val_acc: 0.1185\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 4.5227 - acc: 0.2054 - val_loss: 4.3586 - val_acc: 0.1784\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.1692 - acc: 0.2121 - val_loss: 4.6564 - val_acc: 0.1234\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 3.9131 - acc: 0.2163 - val_loss: 3.8804 - val_acc: 0.1736\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 3.6549 - acc: 0.2165 - val_loss: 3.6603 - val_acc: 0.1923\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 3.5841 - acc: 0.2147 - val_loss: 4.0883 - val_acc: 0.1582\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 3.4073 - acc: 0.2209 - val_loss: 3.6518 - val_acc: 0.1756\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 3.2660 - acc: 0.2222 - val_loss: 3.9649 - val_acc: 0.1307\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 317us/step - loss: 3.1184 - acc: 0.2285 - val_loss: 5.6079 - val_acc: 0.1009\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 16s 317us/step - loss: 3.0639 - acc: 0.2316 - val_loss: 3.3288 - val_acc: 0.1588\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.9872 - acc: 0.2343 - val_loss: 3.0445 - val_acc: 0.1754\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 2.9108 - acc: 0.2347 - val_loss: 3.4813 - val_acc: 0.1629\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 2.9123 - acc: 0.2343 - val_loss: 3.0801 - val_acc: 0.1766\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 2.8144 - acc: 0.2352 - val_loss: 3.5001 - val_acc: 0.1301\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.7547 - acc: 0.2399 - val_loss: 3.2037 - val_acc: 0.1370\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.7302 - acc: 0.2420 - val_loss: 3.0460 - val_acc: 0.1528\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 2.7036 - acc: 0.2361 - val_loss: 2.8811 - val_acc: 0.1966\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 2.7125 - acc: 0.2401 - val_loss: 3.9612 - val_acc: 0.1100\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.7378 - acc: 0.2381 - val_loss: 2.6657 - val_acc: 0.2144\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.6717 - acc: 0.2373 - val_loss: 2.5885 - val_acc: 0.2223\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 2.5579 - acc: 0.2405 - val_loss: 2.8240 - val_acc: 0.1651\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.5198 - acc: 0.2432 - val_loss: 2.5973 - val_acc: 0.1922\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.5609 - acc: 0.2434 - val_loss: 2.7281 - val_acc: 0.1919\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.4991 - acc: 0.2412 - val_loss: 2.6976 - val_acc: 0.2085\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.4678 - acc: 0.2491 - val_loss: 2.5726 - val_acc: 0.2207\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.4442 - acc: 0.2507 - val_loss: 5.1315 - val_acc: 0.1008\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 2.4422 - acc: 0.2470 - val_loss: 2.6159 - val_acc: 0.1892\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.4377 - acc: 0.2505 - val_loss: 3.6224 - val_acc: 0.1184\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.4197 - acc: 0.2500 - val_loss: 3.1883 - val_acc: 0.1179\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.4090 - acc: 0.2515 - val_loss: 4.0928 - val_acc: 0.1064\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.4009 - acc: 0.2541 - val_loss: 2.4022 - val_acc: 0.2293\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3611 - acc: 0.2561 - val_loss: 2.9792 - val_acc: 0.1227\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 2.3853 - acc: 0.2519 - val_loss: 3.8540 - val_acc: 0.1016\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.3839 - acc: 0.2612 - val_loss: 2.8893 - val_acc: 0.1288\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.4208 - acc: 0.2493 - val_loss: 3.7905 - val_acc: 0.1068\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3470 - acc: 0.2556 - val_loss: 2.6423 - val_acc: 0.1637\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3348 - acc: 0.2644 - val_loss: 2.5336 - val_acc: 0.1827\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3206 - acc: 0.2683 - val_loss: 2.3171 - val_acc: 0.2746\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 2.3741 - acc: 0.2608 - val_loss: 5.0005 - val_acc: 0.1018\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 2.3647 - acc: 0.2527 - val_loss: 4.8393 - val_acc: 0.1019\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.3317 - acc: 0.2620 - val_loss: 2.9608 - val_acc: 0.1266\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 2.3585 - acc: 0.2535 - val_loss: 2.4370 - val_acc: 0.1996\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 2.3286 - acc: 0.2647 - val_loss: 2.7330 - val_acc: 0.1439\n",
            "Experiment with Regulizer = 0.000100\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 1,752,234\n",
            "Trainable params: 1,750,250\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 17s 346us/step - loss: 5.7786 - acc: 0.1505 - val_loss: 5.2590 - val_acc: 0.2760\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 318us/step - loss: 5.3670 - acc: 0.2157 - val_loss: 5.0985 - val_acc: 0.2989\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 5.1781 - acc: 0.2541 - val_loss: 5.0041 - val_acc: 0.3059\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 5.0402 - acc: 0.2797 - val_loss: 4.8578 - val_acc: 0.3421\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.9356 - acc: 0.2957 - val_loss: 4.7561 - val_acc: 0.3645\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.8371 - acc: 0.3117 - val_loss: 4.6734 - val_acc: 0.3706\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.7523 - acc: 0.3199 - val_loss: 4.6086 - val_acc: 0.3736\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.6678 - acc: 0.3322 - val_loss: 4.5189 - val_acc: 0.3857\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 4.5887 - acc: 0.3389 - val_loss: 4.4289 - val_acc: 0.3969\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 4.5137 - acc: 0.3514 - val_loss: 4.3681 - val_acc: 0.4040\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 4.4472 - acc: 0.3567 - val_loss: 4.2967 - val_acc: 0.4069\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 4.3815 - acc: 0.3592 - val_loss: 4.2315 - val_acc: 0.4147\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 4.3156 - acc: 0.3655 - val_loss: 4.1880 - val_acc: 0.4095\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 4.2485 - acc: 0.3727 - val_loss: 4.0897 - val_acc: 0.4268\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 4.1894 - acc: 0.3766 - val_loss: 4.0463 - val_acc: 0.4192\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.1301 - acc: 0.3800 - val_loss: 4.0027 - val_acc: 0.4201\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 4.0673 - acc: 0.3833 - val_loss: 3.9362 - val_acc: 0.4293\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 4.0160 - acc: 0.3833 - val_loss: 3.8756 - val_acc: 0.4289\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 3.9659 - acc: 0.3861 - val_loss: 3.8135 - val_acc: 0.4331\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 3.9147 - acc: 0.3870 - val_loss: 3.8403 - val_acc: 0.4109\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 3.8605 - acc: 0.3882 - val_loss: 3.7213 - val_acc: 0.4308\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 3.8052 - acc: 0.3943 - val_loss: 3.7375 - val_acc: 0.4143\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.7446 - acc: 0.3956 - val_loss: 3.6486 - val_acc: 0.4182\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 3.6874 - acc: 0.3965 - val_loss: 3.5862 - val_acc: 0.4385\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 3.6393 - acc: 0.4007 - val_loss: 3.5274 - val_acc: 0.4385\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.5987 - acc: 0.4000 - val_loss: 3.4995 - val_acc: 0.4287\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 3.5511 - acc: 0.4038 - val_loss: 3.4077 - val_acc: 0.4427\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 3.4966 - acc: 0.4054 - val_loss: 3.4016 - val_acc: 0.4270\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 3.4504 - acc: 0.4053 - val_loss: 3.3315 - val_acc: 0.4413\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 3.4082 - acc: 0.4043 - val_loss: 3.3140 - val_acc: 0.4340\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 3.3611 - acc: 0.4066 - val_loss: 3.2730 - val_acc: 0.4357\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 3.3189 - acc: 0.4066 - val_loss: 3.2544 - val_acc: 0.4280\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 3.2745 - acc: 0.4066 - val_loss: 3.1743 - val_acc: 0.4461\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 3.2237 - acc: 0.4116 - val_loss: 3.1156 - val_acc: 0.4468\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 3.1863 - acc: 0.4132 - val_loss: 3.0739 - val_acc: 0.4501\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 3.1512 - acc: 0.4100 - val_loss: 3.0854 - val_acc: 0.4306\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 3.1060 - acc: 0.4132 - val_loss: 3.0190 - val_acc: 0.4487\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 3.0601 - acc: 0.4167 - val_loss: 2.9673 - val_acc: 0.4331\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 3.0143 - acc: 0.4201 - val_loss: 2.9392 - val_acc: 0.4393\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 2.9747 - acc: 0.4198 - val_loss: 2.8691 - val_acc: 0.4537\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 2.9511 - acc: 0.4190 - val_loss: 2.8758 - val_acc: 0.4386\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.9187 - acc: 0.4167 - val_loss: 2.8202 - val_acc: 0.4449\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 2.8653 - acc: 0.4202 - val_loss: 2.7839 - val_acc: 0.4464\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.8334 - acc: 0.4188 - val_loss: 2.7977 - val_acc: 0.4355\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 2.8019 - acc: 0.4195 - val_loss: 2.7404 - val_acc: 0.4328\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 2.7660 - acc: 0.4252 - val_loss: 2.6978 - val_acc: 0.4386\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.7362 - acc: 0.4232 - val_loss: 2.7582 - val_acc: 0.4141\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 2.7205 - acc: 0.4142 - val_loss: 2.6413 - val_acc: 0.4275\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 2.6970 - acc: 0.4136 - val_loss: 2.6573 - val_acc: 0.4183\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 2.6647 - acc: 0.4125 - val_loss: 2.5684 - val_acc: 0.4376\n",
            "Experiment with Regulizer = 0.000000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 1,752,234\n",
            "Trainable params: 1,750,250\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 18s 364us/step - loss: 2.5009 - acc: 0.1531 - val_loss: 2.0931 - val_acc: 0.2323\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 2.1740 - acc: 0.2137 - val_loss: 2.0314 - val_acc: 0.2494\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 2.0535 - acc: 0.2458 - val_loss: 1.9045 - val_acc: 0.2951\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.9813 - acc: 0.2680 - val_loss: 1.8540 - val_acc: 0.3334\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 17s 336us/step - loss: 1.9369 - acc: 0.2832 - val_loss: 1.8178 - val_acc: 0.3415\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 1.8978 - acc: 0.2982 - val_loss: 1.7658 - val_acc: 0.3649\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.8696 - acc: 0.3122 - val_loss: 1.7499 - val_acc: 0.3613\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 1.8431 - acc: 0.3203 - val_loss: 1.7505 - val_acc: 0.3658\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 1.8251 - acc: 0.3270 - val_loss: 1.7024 - val_acc: 0.3867\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.8125 - acc: 0.3362 - val_loss: 1.6984 - val_acc: 0.3878\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.7923 - acc: 0.3444 - val_loss: 1.7013 - val_acc: 0.3875\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 1.7726 - acc: 0.3513 - val_loss: 1.6537 - val_acc: 0.4132\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 1.7640 - acc: 0.3549 - val_loss: 1.6225 - val_acc: 0.4192\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 1.7478 - acc: 0.3614 - val_loss: 1.6241 - val_acc: 0.4228\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.7393 - acc: 0.3704 - val_loss: 1.6111 - val_acc: 0.4262\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.7233 - acc: 0.3758 - val_loss: 1.6156 - val_acc: 0.4261\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 16s 321us/step - loss: 1.7149 - acc: 0.3780 - val_loss: 1.6170 - val_acc: 0.4263\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.7080 - acc: 0.3815 - val_loss: 1.5842 - val_acc: 0.4298\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.6943 - acc: 0.3868 - val_loss: 1.5693 - val_acc: 0.4340\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.6807 - acc: 0.3942 - val_loss: 1.5387 - val_acc: 0.4530\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.6712 - acc: 0.3968 - val_loss: 1.5494 - val_acc: 0.4484\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.6517 - acc: 0.4049 - val_loss: 1.5394 - val_acc: 0.4508\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.6557 - acc: 0.4001 - val_loss: 1.5444 - val_acc: 0.4441\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.6478 - acc: 0.4083 - val_loss: 1.5888 - val_acc: 0.4367\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.6398 - acc: 0.4111 - val_loss: 1.5560 - val_acc: 0.4385\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.6378 - acc: 0.4106 - val_loss: 1.5266 - val_acc: 0.4549\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.6297 - acc: 0.4162 - val_loss: 1.5309 - val_acc: 0.4500\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.6248 - acc: 0.4180 - val_loss: 1.5510 - val_acc: 0.4496\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.6173 - acc: 0.4198 - val_loss: 1.5201 - val_acc: 0.4550\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.6181 - acc: 0.4211 - val_loss: 1.5302 - val_acc: 0.4552\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.6163 - acc: 0.4184 - val_loss: 1.5115 - val_acc: 0.4543\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.6083 - acc: 0.4217 - val_loss: 1.5363 - val_acc: 0.4494\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.6039 - acc: 0.4233 - val_loss: 1.5123 - val_acc: 0.4601\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.6012 - acc: 0.4242 - val_loss: 1.5052 - val_acc: 0.4576\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5961 - acc: 0.4288 - val_loss: 1.4930 - val_acc: 0.4639\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5931 - acc: 0.4284 - val_loss: 1.5339 - val_acc: 0.4510\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.5838 - acc: 0.4331 - val_loss: 1.5049 - val_acc: 0.4579\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5780 - acc: 0.4351 - val_loss: 1.4730 - val_acc: 0.4764\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5692 - acc: 0.4363 - val_loss: 1.4863 - val_acc: 0.4662\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.5646 - acc: 0.4378 - val_loss: 1.5012 - val_acc: 0.4613\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.5713 - acc: 0.4404 - val_loss: 1.4820 - val_acc: 0.4687\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5665 - acc: 0.4383 - val_loss: 1.4822 - val_acc: 0.4720\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 1.5588 - acc: 0.4409 - val_loss: 1.4747 - val_acc: 0.4689\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5567 - acc: 0.4447 - val_loss: 1.4778 - val_acc: 0.4679\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 1.5613 - acc: 0.4432 - val_loss: 1.5199 - val_acc: 0.4524\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 1.5561 - acc: 0.4445 - val_loss: 1.4775 - val_acc: 0.4699\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5502 - acc: 0.4451 - val_loss: 1.5017 - val_acc: 0.4613\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.5484 - acc: 0.4450 - val_loss: 1.5338 - val_acc: 0.4396\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5435 - acc: 0.4515 - val_loss: 1.4784 - val_acc: 0.4698\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 1.5431 - acc: 0.4494 - val_loss: 1.4863 - val_acc: 0.4678\n",
            "Experiment with Regulizer = 0.000000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "hidden_layer4 (Dense)        (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "hidden_layer5 (Dense)        (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 1,752,234\n",
            "Trainable params: 1,750,250\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 2.4948 - acc: 0.1533 - val_loss: 2.0501 - val_acc: 0.2413\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 2.1536 - acc: 0.2170 - val_loss: 1.9422 - val_acc: 0.2786\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 2.0289 - acc: 0.2547 - val_loss: 1.8607 - val_acc: 0.3215\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.9588 - acc: 0.2775 - val_loss: 1.8238 - val_acc: 0.3376\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 1.9126 - acc: 0.2941 - val_loss: 1.7899 - val_acc: 0.3528\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.8790 - acc: 0.3077 - val_loss: 1.7666 - val_acc: 0.3586\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.8510 - acc: 0.3219 - val_loss: 1.7654 - val_acc: 0.3625\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.8292 - acc: 0.3317 - val_loss: 1.7125 - val_acc: 0.3812\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.8055 - acc: 0.3420 - val_loss: 1.6995 - val_acc: 0.3858\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.7831 - acc: 0.3505 - val_loss: 1.6702 - val_acc: 0.4011\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 17s 335us/step - loss: 1.7721 - acc: 0.3540 - val_loss: 1.6465 - val_acc: 0.4121\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.7567 - acc: 0.3608 - val_loss: 1.6239 - val_acc: 0.4144\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 17s 335us/step - loss: 1.7436 - acc: 0.3681 - val_loss: 1.6416 - val_acc: 0.4076\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 17s 335us/step - loss: 1.7342 - acc: 0.3701 - val_loss: 1.6107 - val_acc: 0.4203\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 1.7235 - acc: 0.3761 - val_loss: 1.5906 - val_acc: 0.4294\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 17s 340us/step - loss: 1.7065 - acc: 0.3830 - val_loss: 1.5978 - val_acc: 0.4252\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 17s 333us/step - loss: 1.6972 - acc: 0.3883 - val_loss: 1.5919 - val_acc: 0.4329\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.6948 - acc: 0.3897 - val_loss: 1.6019 - val_acc: 0.4249\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.6834 - acc: 0.3946 - val_loss: 1.5787 - val_acc: 0.4348\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 17s 330us/step - loss: 1.6777 - acc: 0.3958 - val_loss: 1.5797 - val_acc: 0.4326\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.6722 - acc: 0.3991 - val_loss: 1.5512 - val_acc: 0.4514\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.6571 - acc: 0.4031 - val_loss: 1.5516 - val_acc: 0.4419\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.6541 - acc: 0.4024 - val_loss: 1.5593 - val_acc: 0.4334\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.6435 - acc: 0.4087 - val_loss: 1.5280 - val_acc: 0.4503\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.6364 - acc: 0.4121 - val_loss: 1.5602 - val_acc: 0.4437\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.6356 - acc: 0.4131 - val_loss: 1.5311 - val_acc: 0.4510\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.6269 - acc: 0.4141 - val_loss: 1.5177 - val_acc: 0.4518\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.6212 - acc: 0.4179 - val_loss: 1.5521 - val_acc: 0.4417\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.6243 - acc: 0.4184 - val_loss: 1.5269 - val_acc: 0.4557\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.6207 - acc: 0.4193 - val_loss: 1.5114 - val_acc: 0.4539\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.6161 - acc: 0.4196 - val_loss: 1.5547 - val_acc: 0.4378\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.6098 - acc: 0.4233 - val_loss: 1.5142 - val_acc: 0.4545\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.6034 - acc: 0.4250 - val_loss: 1.5373 - val_acc: 0.4428\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 17s 332us/step - loss: 1.5936 - acc: 0.4300 - val_loss: 1.5081 - val_acc: 0.4593\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 17s 334us/step - loss: 1.5938 - acc: 0.4288 - val_loss: 1.5035 - val_acc: 0.4581\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 16s 327us/step - loss: 1.5829 - acc: 0.4331 - val_loss: 1.5093 - val_acc: 0.4554\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.5818 - acc: 0.4368 - val_loss: 1.4976 - val_acc: 0.4583\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.5747 - acc: 0.4355 - val_loss: 1.4858 - val_acc: 0.4669\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 16s 323us/step - loss: 1.5717 - acc: 0.4368 - val_loss: 1.4881 - val_acc: 0.4640\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.5619 - acc: 0.4424 - val_loss: 1.4953 - val_acc: 0.4613\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.5681 - acc: 0.4429 - val_loss: 1.4883 - val_acc: 0.4615\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 16s 330us/step - loss: 1.5563 - acc: 0.4444 - val_loss: 1.4849 - val_acc: 0.4646\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 17s 331us/step - loss: 1.5607 - acc: 0.4443 - val_loss: 1.4772 - val_acc: 0.4682\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 16s 329us/step - loss: 1.5563 - acc: 0.4465 - val_loss: 1.4706 - val_acc: 0.4691\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 1.5481 - acc: 0.4477 - val_loss: 1.4901 - val_acc: 0.4640\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 16s 324us/step - loss: 1.5339 - acc: 0.4522 - val_loss: 1.4650 - val_acc: 0.4796\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.5381 - acc: 0.4547 - val_loss: 1.4715 - val_acc: 0.4738\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.5330 - acc: 0.4551 - val_loss: 1.4824 - val_acc: 0.4699\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 16s 325us/step - loss: 1.5270 - acc: 0.4565 - val_loss: 1.4704 - val_acc: 0.4744\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 16s 328us/step - loss: 1.5189 - acc: 0.4599 - val_loss: 1.4633 - val_acc: 0.4773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7dpckqIAujz",
        "colab_type": "code",
        "outputId": "8799e379-d993-4748-9921-dc97911b9a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\"\"\"Code Here\n",
        "將結果繪出\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n",
        "plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n",
        "plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT2igK6WUn9BAV0opP/H/A+d3RsTmiN7oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5eM06JVSysdp0CullI/7f1XFQ8Tvr3gMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GSIOHVCA7WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}